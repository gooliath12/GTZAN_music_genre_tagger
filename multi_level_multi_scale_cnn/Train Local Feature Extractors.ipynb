{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a set of CNNs handling a inputs (segments of mel-spectrograms) in different scale. \n",
    "\n",
    "These CNN models will be used as loacal feature extrators in music genre tagger.\n",
    "\n",
    "We plan to train 5 different scales of CNNs. Their segment length are: 20, 30, 60, 120, 240.\n",
    "\n",
    "All CNNs contain 5 conv layers. The \"vertical\" pooling size in each layer would be: `[2, 3, 2, 2, 4]`.\n",
    "\n",
    "The horizontal pooling sizes are set according to their segment length:\n",
    "\n",
    "Segment Length | Pooling sizes\n",
    "- | -\n",
    "20 | `[2, 2, 2, 2, 1]`\n",
    "30 | `[2, 2, 2, 2, 1]`\n",
    "60 | `[3, 2, 2, 2, 2]`\n",
    "120 | `[4, 3, 2, 2, 2]`\n",
    "240 | `[4, 4, 3, 2, 2]`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from keras.utils.visualize_util import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train these local feature extractors on GTZAN dataset. \n",
    "\n",
    "Audio files are first preprocessed to 2D mel-spectrograms then partitioned horizontally in different segment lengths. All data preprocessing jobs are done in `Split Dataset.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "X = np.load('/Users/pengguo/Desktop/coms4995/Project/Multi_scale/dataset/X_train_seg30.npy')\n",
    "Y_pre = np.load('/Users/pengguo/Desktop/coms4995/Project/Multi_scale/dataset/Y_train_seg30.npy').astype(int)\n",
    "\n",
    "# Encode Y_pre to one-hot(Y)\n",
    "Y = np.zeros((Y_pre.shape[0], 10))\n",
    "Y[np.arange(Y_pre.shape[0]), Y_pre] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (40500, 1, 96, 30)\n",
      "Shape of Y: (40500, 10)\n"
     ]
    }
   ],
   "source": [
    "print \"Shape of X: {}\".format(X.shape)\n",
    "print \"Shape of Y: {}\".format(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_model(seg_length, pool_sizes_hori):\n",
    "    '''\n",
    "    Generate model with different scales.\n",
    "    seg_length and pooling layer sizes are set adjusting to different scales.\n",
    "    '''\n",
    "    psh = [0] + pool_sizes_hori # padding at front for index alignment\n",
    "    \n",
    "    # input\n",
    "    x = Input(shape=(1, 96, seg_length))\n",
    "\n",
    "    # 1st conv layer\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv1_{}'.format(seg_length))(x)\n",
    "    conv1 = BatchNormalization(axis=1, mode=2, name='BN1_{}'.format(seg_length))(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, psh[1]))(conv1)\n",
    "\n",
    "    # 2nd conv layer\n",
    "    conv2 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv2_{}'.format(seg_length))(conv1)\n",
    "    conv2 = BatchNormalization(axis=1, mode=2, name='BN2_{}'.format(seg_length))(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(3, psh[2]))(conv2)\n",
    "\n",
    "    # 3rd conv layer\n",
    "    conv3 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv3_{}'.format(seg_length))(conv2)\n",
    "    conv3 = BatchNormalization(axis=1, mode=2, name='BN3_{}'.format(seg_length))(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, psh[3]))(conv3)\n",
    "\n",
    "    # 4th conv layer\n",
    "    conv4 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv4_{}'.format(seg_length))(conv3)\n",
    "    conv4 = BatchNormalization(axis=1, mode=2, name='BN4_{}'.format(seg_length))(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv4)\n",
    "    conv4 = MaxPooling2D(pool_size=(2, psh[4]))(conv4)\n",
    "\n",
    "    # 5th conv layer\n",
    "    conv5 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv5_{}'.format(seg_length))(conv4)\n",
    "    conv5 = BatchNormalization(axis=1, mode=2, name='BN5_{}'.format(seg_length))(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv5)\n",
    "    conv5 = MaxPooling2D(pool_size=(4, psh[5]))(conv5)\n",
    "\n",
    "    # Flatten the output of last conv layer (conv5)\n",
    "    conv5 = Flatten()(conv5)\n",
    "    \n",
    "    # output layer\n",
    "    out = Dense(10, input_shape=[32], activation='softmax')(conv5)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(input=x, output=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "seg_length = 30\n",
    "pool_sizes_hori = [2, 2, 2, 2, 1] # sizes of pooling layers (in horizontal direction)\n",
    "\n",
    "# Generate model\n",
    "model = gen_model(seg_length, pool_sizes_hori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 1, 96, 30)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1_30 (Convolution2D)         (None, 32, 96, 30)    320         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "BN1_30 (BatchNormalization)      (None, 32, 96, 30)    128         conv1_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_1 (ELU)                      (None, 32, 96, 30)    0           BN1_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 32, 48, 15)    0           elu_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_30 (Convolution2D)         (None, 32, 48, 15)    9248        maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "BN2_30 (BatchNormalization)      (None, 32, 48, 15)    128         conv2_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_2 (ELU)                      (None, 32, 48, 15)    0           BN2_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 32, 16, 7)     0           elu_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv3_30 (Convolution2D)         (None, 32, 16, 7)     9248        maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "BN3_30 (BatchNormalization)      (None, 32, 16, 7)     128         conv3_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_3 (ELU)                      (None, 32, 16, 7)     0           BN3_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 32, 8, 3)      0           elu_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv4_30 (Convolution2D)         (None, 32, 8, 3)      9248        maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "BN4_30 (BatchNormalization)      (None, 32, 8, 3)      128         conv4_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_4 (ELU)                      (None, 32, 8, 3)      0           BN4_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 32, 4, 1)      0           elu_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv5_30 (Convolution2D)         (None, 32, 4, 1)      9248        maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "BN5_30 (BatchNormalization)      (None, 32, 4, 1)      128         conv5_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "elu_5 (ELU)                      (None, 32, 4, 1)      0           BN5_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 32, 1, 1)      0           elu_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 32)            0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 10)            330         flatten_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 38,282\n",
      "Trainable params: 37,962\n",
      "Non-trainable params: 320\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Save all \"model history\"s to a list for plotting the whole training process.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split Dataset (90% train + 10% dev)\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36450 samples, validate on 4050 samples\n",
      "Epoch 1/2\n",
      "36450/36450 [==============================] - 428s - loss: 1.9833 - acc: 0.2853 - val_loss: 1.6963 - val_acc: 0.4096\n",
      "Epoch 2/2\n",
      "36450/36450 [==============================] - 413s - loss: 1.5191 - acc: 0.4733 - val_loss: 1.4034 - val_acc: 0.5212\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "model_his = model.fit(X_train, Y_train, batch_size=256, validation_data=(X_dev, Y_dev), nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "model_eval = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model weights\n",
    "import os\n",
    "if not os.path.exists('./weights/'):\n",
    "    os.mkdir('./weights/')\n",
    "model.save_weights('./weights/cnn_{}.h5'.format(seg_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Model weights\n",
    "if not os.path.exists('./weights/'):\n",
    "    os.mkdir('./weights/')\n",
    "model.load_weights('./weights/local_cnn_{}.h5'.format(seg_length), by_name=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
