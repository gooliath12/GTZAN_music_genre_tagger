{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Local Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a set of CNNs handling a inputs (segments of mel-spectrograms) in different scale. These CNN models will be used as *local feature extrators* in music genre tagger.\n",
    "\n",
    "In this project we trained 4 different scales of CNNs. Their segment length are: 30, 60, 120, 240. Structures of all 4 CNNs are very similar to the model we used in \"multi-level CNN\" part, except that models here only contain 4 conv layers. The \"vertical\" pooling size in each layer would be: `[2, 3, 2, 2]`. The horizontal pooling sizes are set according to their segment length:\n",
    "\n",
    "Segment Length | Pooling sizes\n",
    "- | -\n",
    "30 | `[2, 2, 2, 2]`\n",
    "60 | `[3, 2, 2, 2]`\n",
    "120 | `[3, 3, 2, 2]`\n",
    "240 | `[4, 4, 3, 2]`\n",
    "\n",
    "The data loading and model training process for these 4 models are seperated into 4 sections below. To train models, run all cells under each section and use cells in **\"Helper Cells\" section ** for saving/loading model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"floatX=float32,device=cuda,exception_verbosity=high\"\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Input, Dense, merge, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below provides a \"templete\" for defining CNN models used as local feature extractors. All 4 CNN models are identical except the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_model_3(seg_length, pool_sizes_hori):\n",
    "    '''\n",
    "    Generate model with different scales.\n",
    "    seg_length and pooling layer sizes are set adjusting to different scales.\n",
    "    '''\n",
    "    psh = [0] + pool_sizes_hori # padding at front for index alignment\n",
    "    \n",
    "    # input\n",
    "    x = Input(shape=(1, 96, seg_length))\n",
    "\n",
    "    # 1st conv layer\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv1_{}'.format(seg_length))(x)\n",
    "    conv1 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv1)\n",
    "    conv1 = MaxPooling2D(pool_size=(2, psh[1]))(conv1)\n",
    "    conv1 = Dropout(0.2)(conv1)\n",
    "\n",
    "    # 2nd conv layer\n",
    "    conv2 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv2_{}'.format(seg_length))(conv1)\n",
    "    conv2 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv2)\n",
    "    conv2 = MaxPooling2D(pool_size=(3, psh[2]))(conv2)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    \n",
    "    # 3rd conv layer\n",
    "    conv3 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv3_{}'.format(seg_length))(conv2)\n",
    "    conv3 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv3)\n",
    "    conv3 = MaxPooling2D(pool_size=(2, psh[3]))(conv3)\n",
    "    conv3 = Dropout(0.2)(conv3)\n",
    "    \n",
    "    # 4th conv layer\n",
    "    conv4 = Convolution2D(32, 3, 3, border_mode='same', init='he_normal', name='conv4_{}'.format(seg_length))(conv3)\n",
    "    conv4 = keras.layers.advanced_activations.ELU(alpha=1.0)(conv4)\n",
    "    conv4 = MaxPooling2D(pool_size=(2, psh[4]))(conv4)\n",
    "    \n",
    "    # Flatten the output of last conv layer (conv5)\n",
    "    flat = Flatten()(conv4)\n",
    "    \n",
    "    # Dense layer # 1\n",
    "    dense1 = Dense(64, activation='relu')(flat)\n",
    "    dense1 = Dropout(0.2)(dense1)\n",
    "    \n",
    "    # output layer\n",
    "    out = Dense(10, activation='softmax')(dense1)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(input=x, output=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells in this section are named \"helper cells\" because functions such as save/load model, save/load model weights, and save/load model training histories are all implemented here. \n",
    "\n",
    "Before / after training of each model, run cells below for saving / loading the data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save Model (for continue training) ###\n",
    "model.save('./models/cnn_{}.h5'.format(seg_length))  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load previously trained model ###\n",
    "# model = load_model('./models/cnn_{}.h5'.format(seg_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save Training History (for plotting) ###\n",
    "import pickle, datetime\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "with open('./train_logs/train_his_{}_{}'.format(seg_length, now), 'wb') as file_pi:\n",
    "    pickle.dump(model_his.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save Model weights (for future transferr) ###\n",
    "if not os.path.exists('./weights/'):\n",
    "    os.mkdir('./weights/')\n",
    "model.save_weights('./weights/local_cnn_4_{}.h5'.format(seg_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load Model weights ###\n",
    "if not os.path.exists('./weights/'):\n",
    "    os.mkdir('./weights/')\n",
    "model.load_weights('./weights/local_cnn__4_{}.h5'.format(seg_length), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with seg_length 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "seg_length = 30\n",
    "pool_sizes_hori = [2, 2, 2, 2] # sizes of pooling layers (in horizontal direction)\n",
    "\n",
    "# Load Training Data\n",
    "X_train = np.load('./dataset/X_train_seg{}.npy'.format(seg_length))\n",
    "Y_train_pre = np.load('./dataset/Y_train_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_train = np.zeros((Y_train_pre.shape[0], 10))\n",
    "Y_train[np.arange(Y_train_pre.shape[0]), Y_train_pre] = 1\n",
    "\n",
    "# Load Test Data\n",
    "X_test = np.load('./dataset/X_test_seg{}.npy'.format(seg_length))\n",
    "Y_test_pre = np.load('./dataset/Y_test_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_test = np.zeros((Y_test_pre.shape[0], 10))\n",
    "Y_test[np.arange(Y_test_pre.shape[0]), Y_test_pre] = 1\n",
    "\n",
    "# Generate model\n",
    "model_30 = gen_model_3(seg_length, pool_sizes_hori)\n",
    "\n",
    "# # Load Model Weights\n",
    "# model_30.load_weights('./weights/local_cnn_{}.h5'.format(seg_length), by_name=True)\n",
    "\n",
    "# Compile model\n",
    "model_30.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model_his = model_30.fit(X_train, Y_train, batch_size=256, validation_data=(X_test, Y_test), nb_epoch=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with seg_length 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "seg_length = 60\n",
    "pool_sizes_hori = [3, 3, 2, 2] # sizes of pooling layers (in horizontal direction)\n",
    "\n",
    "# Load Training Data\n",
    "X_train = np.load('./dataset/X_train_seg{}.npy'.format(seg_length))\n",
    "Y_train_pre = np.load('./dataset/Y_train_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_train = np.zeros((Y_train_pre.shape[0], 10))\n",
    "Y_train[np.arange(Y_train_pre.shape[0]), Y_train_pre] = 1\n",
    "\n",
    "# Load Test Data\n",
    "X_test = np.load('./dataset/X_test_seg{}.npy'.format(seg_length))\n",
    "Y_test_pre = np.load('./dataset/Y_test_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_test = np.zeros((Y_test_pre.shape[0], 10))\n",
    "Y_test[np.arange(Y_test_pre.shape[0]), Y_test_pre] = 1\n",
    "\n",
    "# Generate model\n",
    "model = gen_model_3(seg_length, pool_sizes_hori)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model_his = model.fit(X_train, Y_train, batch_size=256, validation_data=(X_test, Y_test), nb_epoch=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with seg_length 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "seg_length = 120\n",
    "pool_sizes_hori = [3, 3, 2, 2] # sizes of pooling layers (in horizontal direction)\n",
    "\n",
    "# Load Training Data\n",
    "X_train = np.load('./dataset/X_train_seg{}.npy'.format(seg_length))\n",
    "Y_train_pre = np.load('./dataset/Y_train_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_train = np.zeros((Y_train_pre.shape[0], 10))\n",
    "Y_train[np.arange(Y_train_pre.shape[0]), Y_train_pre] = 1\n",
    "\n",
    "# Load Test Data\n",
    "X_test = np.load('./dataset/X_test_seg{}.npy'.format(seg_length))\n",
    "Y_test_pre = np.load('./dataset/Y_test_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_test = np.zeros((Y_test_pre.shape[0], 10))\n",
    "Y_test[np.arange(Y_test_pre.shape[0]), Y_test_pre] = 1\n",
    "\n",
    "# Generate model\n",
    "model = gen_model_3(seg_length, pool_sizes_hori)\n",
    "\n",
    "# Load Model\n",
    "# model = load_model('./models/cnn_{}.h5'.format(seg_length))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model_his = model.fit(X_train, Y_train, batch_size=256, validation_data=(X_test, Y_test), nb_epoch=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with seg_length 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "seg_length = 240\n",
    "pool_sizes_hori = [4, 4, 3, 2] # sizes of pooling layers (in horizontal direction)\n",
    "\n",
    "# Load Training Data\n",
    "X_train = np.load('./dataset/X_train_seg{}.npy'.format(seg_length))\n",
    "Y_train_pre = np.load('./dataset/Y_train_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_train = np.zeros((Y_train_pre.shape[0], 10))\n",
    "Y_train[np.arange(Y_train_pre.shape[0]), Y_train_pre] = 1\n",
    "\n",
    "# Load Test Data\n",
    "X_test = np.load('./dataset/X_test_seg{}.npy'.format(seg_length))\n",
    "Y_test_pre = np.load('./dataset/Y_test_seg{}.npy'.format(seg_length)).astype(int)\n",
    "Y_test = np.zeros((Y_test_pre.shape[0], 10))\n",
    "Y_test[np.arange(Y_test_pre.shape[0]), Y_test_pre] = 1\n",
    "\n",
    "# Generate model\n",
    "model = gen_model_3(seg_length, pool_sizes_hori)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "model_his = model.fit(X_train, Y_train, batch_size=256, validation_data=(X_test, Y_test), nb_epoch=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
